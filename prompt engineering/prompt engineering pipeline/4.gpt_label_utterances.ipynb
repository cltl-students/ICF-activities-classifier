{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "# import IPython\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst, val): # code from https://www.geeksforgeeks.org/python-split-list-into-lists-by-particular-value/\n",
    "    return [list(group) for k, group in itertools.groupby(lst, lambda x: x==val) if not k]\n",
    "\n",
    "def convert_format_for_gptlabel(filepath):\n",
    "    \"\"\"convert tsv to the conversation format to feed to gpt to get the category labels...\"\"\"\n",
    "    df = pd.read_csv(filepath, sep = \"\\t\")\n",
    "    l_u_sep = df[\"utterances\"].to_list()\n",
    "    lst_cons = split_list(l_u_sep,\"-\"*20)\n",
    "    conversations = []\n",
    "    for con in lst_cons:\n",
    "        count = 0\n",
    "        processed = \"\"\n",
    "        for utterance in con:\n",
    "            count += 1\n",
    "            utterance = str(count) + \". \" + utterance +'\\n'\n",
    "            processed += utterance\n",
    "        conversations.append(processed)\n",
    "        \n",
    "    return conversations, l_u_sep\n",
    "\n",
    "def gpt_label_zeroshot(conversation, MODEL = \"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    This is to generate dialogues acccording to categories in ICF by feeding the definitions of the each category to the prompt\n",
    "    \"\"\"\n",
    "    # count = 0\n",
    "    query = [\n",
    "            # {\"role\": \"system\", \"content\": \"Think as you are an expert who is very proficient in ICF (International Classification of Functioning, Disability and Health).\"}, # Human\n",
    "            {\"role\": \"system\", \"content\": \"\"\"Pick one most likely label for each utterance in the conversation with subcategories of activities and participation in ICF. The labels includes: \n",
    "             - learning and applying knowledge, \n",
    "             - general tasks and demands, \n",
    "             - communication, \n",
    "             - mobility, \n",
    "             - self-care, \n",
    "             - domestic life areas, \n",
    "             - interpersonal interactions and relationships, \n",
    "             - major life areas, \n",
    "             - community, social and civic life , \n",
    "             - none: if the utterance does not belong to any of the categories above, label it as 'none'\n",
    "            \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"The format of the label is like:\n",
    "            1. label\n",
    "            2. label\n",
    "            3. label\n",
    "                ...\n",
    "            \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Conversation:{conversation}\"},\n",
    "            {\"role\": \"user\", \"content\": \"Let's think utterance by utterance.\"},\n",
    "            {\"role\": \"user\", \"content\": \"labels:\"},\n",
    "            ]\n",
    "        # MODEL = \"gpt-3.5-turbo\"\n",
    "    response_query = openai.ChatCompletion.create(\n",
    "        model=MODEL,\n",
    "        messages=query,\n",
    "        temperature=0,\n",
    "        max_tokens = 500\n",
    "        )\n",
    "\n",
    "    return response_query\n",
    "        \n",
    "def label_multiple_conversations(conversations):\n",
    "    \"\"\"\"\"\"\n",
    "    raw_labels = []\n",
    "    count_con = 0\n",
    "    for c in conversations:\n",
    "        response = gpt_label_zeroshot(c)\n",
    "        raw_labels.append(response.choices[0]['message']['content'])\n",
    "        count_con += 1\n",
    "        print(f\"finish {count_con} conversations\", end=\"\\r\")\n",
    "        time.sleep(18)\n",
    "    print(f\"finish {count_con} conversations\")\n",
    "\n",
    "    return raw_labels\n",
    "\n",
    "def process_raw_labels(raw_labels):\n",
    "    \"\"\"\"\"\"\n",
    "    labels = []\n",
    "    for l in raw_labels:\n",
    "        lst_index_labels = l.split(\"\\n\")\n",
    "        for index_label in lst_index_labels:\n",
    "            index_label = index_label.split(\". \")\n",
    "            label = index_label[1]\n",
    "            labels.append(label)\n",
    "        labels.append(\"-\"*20)\n",
    "\n",
    "    return labels\n",
    "\n",
    "def combine_labels_conversations(l_u_sep, labels, filepath):\n",
    "    \"\"\"\"\"\"\n",
    "    if len(l_u_sep) == len(labels):\n",
    "        print(f\"num of con matches num of labels, wrote to {filepath}\")\n",
    "        df_train = pd.DataFrame({\n",
    "            \"utterances\": l_u_sep,\n",
    "            \"labels\": labels\n",
    "        })\n",
    "        df_train.to_csv(filepath,sep=\"\\t\",index=False)\n",
    "    else:\n",
    "        print(\"ERROR: num of con DOES NOT match num of labels\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "processing data in ./files_to_ready_to_label/trial annotation_2.tsv\n",
      "finish 5 conversations\n",
      "num of con matches num of labels, wrote to ./train_data/trial annotation_2.tsv\n"
     ]
    }
   ],
   "source": [
    "dir = '../response_data/'\n",
    "files = glob.glob(f\"{dir}files_to_ready_to_label/*.tsv\")\n",
    "# time.sleep(60)\n",
    "print(\"start\")\n",
    "for f in files:\n",
    "    print(f\"processing data in {f}\")\n",
    "    conversations, l_u_sep = convert_format_for_gptlabel(f)\n",
    "    raw_labels = label_multiple_conversations(conversations)\n",
    "    labels = process_raw_labels(raw_labels)\n",
    "    filecomponents = f.split(\"/\")\n",
    "    filename = filecomponents[-1]\n",
    "    filepath = f\"{dir}train_data/small_category/{filename}\"\n",
    "    combine_labels_conversations(l_u_sep, labels, filepath)\n",
    "    time.sleep(60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
